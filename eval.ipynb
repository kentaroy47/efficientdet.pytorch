{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2  # OpenCVライブラリ\n",
    "\n",
    "import matplotlib.pyplot as plt \n",
    "%matplotlib inline\n",
    "\n",
    "import torch\n",
    "\n",
    "import pickle\n",
    "\n",
    "import torch.utils.data as data\n",
    "from itertools import product as product\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import dataset\n",
    "from utils.dataset import VOCDataset, DatasetTransform, make_datapath_list, Anno_xml2list, od_collate_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set your VOCdevkit path!\n",
    "vocpath = \"../VOCdevkit/VOC2007\"\n",
    "DEVKIT_PATH = \"../VOCdevkit/\"\n",
    "SET = \"test\"\n",
    "train_img_list, train_anno_list, val_img_list, val_anno_list = make_datapath_list(vocpath)\n",
    "\n",
    "model=\"efficientdet\"\n",
    "backbone = \"efficientnet-b0\"\n",
    "scale = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'../VOCdevkit/VOC2007/JPEGImages/000001.jpg'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_img_list[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'000001'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_index = []\n",
    "for l in val_img_list:\n",
    "    image_index.append(l[-10:-4])\n",
    "\n",
    "image_index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = ['aeroplane', 'bicycle', 'bird', 'boat',\n",
    "               'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "               'cow', 'diningtable', 'dog', 'horse',\n",
    "               'motorbike', 'person', 'pottedplant',\n",
    "               'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "color_mean = (104, 117, 123)  # (BGR)の色の平均値\n",
    "input_size = 300  # 画像のinputサイズを300×300にする\n",
    "\n",
    "## DatasetTransformを適応\n",
    "transform = DatasetTransform(input_size, color_mean)\n",
    "transform_anno = Anno_xml2list(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataset = VOCDataset(val_img_list, val_anno_list, phase=\"val\", transform=DatasetTransform(\n",
    "    input_size, color_mean), transform_anno=Anno_xml2list(class_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_dataloader = data.DataLoader(\n",
    "    val_dataset, batch_size=1, shuffle=False, collate_fn=od_collate_fn, num_workers=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set up model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b0\n",
      "loaded the trained weights\n",
      "using: cuda:0\n"
     ]
    }
   ],
   "source": [
    "if model==\"SSD\":\n",
    "    from utils.ssd_model import SSD\n",
    "elif model==\"retina\":\n",
    "    from utils.retinanet import RetinaFPN as SSD\n",
    "    from utils.retinanet import Bottleneck\n",
    "elif model==\"efficientdet\":\n",
    "    from utils.efficientdet import EfficientDet\n",
    "    \n",
    "voc_classes = ['aeroplane', 'bicycle', 'bird', 'boat',\n",
    "               'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "               'cow', 'diningtable', 'dog', 'horse',\n",
    "               'motorbike', 'person', 'pottedplant',\n",
    "               'sheep', 'sofa', 'train', 'tvmonitor']\n",
    "\n",
    "if scale==1:\n",
    "    ssd_cfg = {\n",
    "        'num_classes': 21,  # 背景クラスを含めた合計クラス数\n",
    "        'input_size': 300*scale,  # 画像の入力サイズ\n",
    "        'bbox_aspect_num': [4, 6, 6, 6, 4, 4],  # 出力するDBoxのアスペクト比の種類\n",
    "        'feature_maps': [37, 18, 9, 5, 3, 1],  # 各sourceの画像サイズ\n",
    "        'steps': [8, 16, 32, 64, 100, 300],  # DBOXの大きさを決める\n",
    "        'min_sizes': [30, 60, 111, 162, 213, 264],  # DBOXの大きさを決める\n",
    "        'max_sizes': [60, 111, 162, 213, 264, 315],  # DBOXの大きさを決める\n",
    "        'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
    "    }\n",
    "elif scale==2:\n",
    "    ssd_cfg = {\n",
    "        'num_classes': 21,  # 背景クラスを含めた合計クラス数\n",
    "        'input_size': 300*scale,  # 画像の入力サイズ\n",
    "        'bbox_aspect_num': [4, 6, 6, 6, 4, 4],  # 出力するDBoxのアスペクト比の種類\n",
    "        'feature_maps': [75, 38, 19, 10, 5, 3],  # 各sourceの画像サイズ\n",
    "        'steps': [8, 16, 32, 64, 100, 300],  # DBOXの大きさを決める\n",
    "        'min_sizes': [30, 60, 111, 162, 213, 264]*scale,  # DBOXの大きさを決める\n",
    "        'max_sizes': [60, 111, 162, 213, 264, 315]*scale,  # DBOXの大きさを決める\n",
    "        'aspect_ratios': [[2], [2, 3], [2, 3], [2, 3], [2], [2]],\n",
    "    }\n",
    "\n",
    "# SSDネットワークモデル\n",
    "if model==\"SSD\":\n",
    "    net = SSD(phase=\"inference\", cfg=ssd_cfg).eval()\n",
    "    net_weights = torch.load('./weights/ssd300_200.pth',\n",
    "                         map_location={'cuda:0': 'cpu'})\n",
    "elif model==\"retina\":\n",
    "    net = SSD(Bottleneck, [2,2,2,2], phase=\"inference\", cfg=ssd_cfg, model=backbone).to(\"cuda\")\n",
    "    net_weights = torch.load('./weights/retinanet300_200.pth',\n",
    "                         map_location={'cuda:0': 'cpu'})\n",
    "else:\n",
    "    net = EfficientDet(phase=\"inference\", cfg=ssd_cfg, verbose=False, backbone=backbone)\n",
    "    net_weights = torch.load('./weights/retinanet300_170.pth',\n",
    "                         map_location={'cuda:0': 'cpu'})\n",
    "\n",
    "net.load_state_dict(net_weights)\n",
    "\n",
    "print('loaded the trained weights')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"using:\", device)\n",
    "\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_imgs = []\n",
    "classes = {}\n",
    "bbox_threshold = 0.05\n",
    "\n",
    "# define detections\n",
    "all_boxes = [[[] for _ in range(len(val_img_list))]\n",
    "               for _ in range(21)]\n",
    "empty_array = np.transpose(np.array([[],[],[],[],[]]), (1,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from utils.ssd_predict_show import SSDPredictShow\n",
    "ssd = SSDPredictShow(eval_categories=voc_classes, net=net, device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# infer images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#all_boxes = ssd.ssd_inference(val_dataloader, all_boxes, data_confidence_level=bbox_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['../VOCdevkit/VOC2007/JPEGImages/000001.jpg',\n",
       " '../VOCdevkit/VOC2007/JPEGImages/000002.jpg',\n",
       " '../VOCdevkit/VOC2007/JPEGImages/000003.jpg',\n",
       " '../VOCdevkit/VOC2007/JPEGImages/000004.jpg',\n",
       " '../VOCdevkit/VOC2007/JPEGImages/000006.jpg',\n",
       " '../VOCdevkit/VOC2007/JPEGImages/000008.jpg',\n",
       " '../VOCdevkit/VOC2007/JPEGImages/000010.jpg',\n",
       " '../VOCdevkit/VOC2007/JPEGImages/000011.jpg',\n",
       " '../VOCdevkit/VOC2007/JPEGImages/000013.jpg',\n",
       " '../VOCdevkit/VOC2007/JPEGImages/000014.jpg']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_img_list[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 256, 37, 37])\n",
      "torch.Size([1, 256, 18, 18])\n",
      "torch.Size([1, 256, 9, 9])\n",
      "torch.Size([1, 256, 5, 5])\n",
      "torch.Size([1, 256, 3, 3])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:2457: UserWarning: nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\n",
      "  warnings.warn(\"nn.functional.upsample is deprecated. Use nn.functional.interpolate instead.\")\n",
      "/home/ubuntu/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/functional.py:2539: UserWarning: Default upsampling behavior when mode=bilinear is changed to align_corners=False since 0.4.0. Please specify align_corners=True if the old behavior is desired. See the documentation of nn.Upsample for details.\n",
      "  \"See the documentation of nn.Upsample for details.\".format(mode))\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Expected object of backend CUDA but got backend CPU for argument #2 'weight'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-df62229fcdc8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_img_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mdetections\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpre_dict_label_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mssd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mssd_predict2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_confidence_level\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.05\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mcls\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m21\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mbox\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/efficientdet.pytorch/utils/ssd_predict_show.py\u001b[0m in \u001b[0;36mssd_predict2\u001b[0;34m(self, image_file_path, data_confidence_level)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    142\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 143\u001b[0;31m             \u001b[0mdetections\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    144\u001b[0m         \u001b[0;31m# detectionsの形は、torch.Size([1, 21, 200, 5])  ※200はtop_kの値\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/efficientdet.pytorch/utils/efficientdet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0mp3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_upsample_add\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlatlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# 38x38\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0msources\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mp3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp6\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp7\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 126\u001b[0;31m             \u001b[0msources\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBiFPN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msources\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m         \u001b[0;31m# look at source size\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/efficientdet.pytorch/BiFPN.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m     17\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0mP7_up\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mConv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0min_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout_channels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_channels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstride\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_channels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mP7_in\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m         \u001b[0mscale\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mP6_in\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mP7_up\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     90\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     91\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 92\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     93\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    491\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    494\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/pytorch_p36/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    336\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[1;32m    337\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 338\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Expected object of backend CUDA but got backend CPU for argument #2 'weight'"
     ]
    }
   ],
   "source": [
    "for i, imp in enumerate(val_img_list):\n",
    "    detections, pre_dict_label_index = ssd.ssd_predict2(imp, data_confidence_level=0.05)\n",
    "    \n",
    "    for cls in range(21):\n",
    "        box = []\n",
    "        for j,pred in enumerate(detections):\n",
    "            if cls == pre_dict_label_index[j]:\n",
    "                box.append(pred)\n",
    "        if not box == []:\n",
    "            all_boxes[cls][i] = box\n",
    "        else:\n",
    "            all_boxes[cls][i] = empty_array\n",
    "    if i%1000==0:\n",
    "        print(\"iter:\", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_boxes[7][0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# eval accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval function\n",
    "def voc_eval(detpath,\n",
    "             annopath,\n",
    "             imagesetfile,\n",
    "             classname,\n",
    "             cachedir,\n",
    "             ovthresh=0.5,\n",
    "             use_07_metric=False):\n",
    "  \"\"\"\n",
    "  rec, prec, ap = voc_eval(detpath,\n",
    "                              annopath,\n",
    "                              imagesetfile,\n",
    "                              classname,\n",
    "                              [ovthresh],\n",
    "                              [use_07_metric])\n",
    "  Top level function that does the PASCAL VOC evaluation.\n",
    "  detpath: Path to detections\n",
    "      detpath.format(classname) should produce the detection results file.\n",
    "  annopath: Path to annotations\n",
    "      annopath.format(imagename) should be the xml annotations file.\n",
    "  imagesetfile: Text file containing the list of images, one image per line.\n",
    "  classname: Category name (duh)\n",
    "  cachedir: Directory for caching the annotations\n",
    "  [ovthresh]: Overlap threshold (default = 0.5)\n",
    "  [use_07_metric]: Whether to use VOC07's 11 point AP computation\n",
    "      (default False)\n",
    "  \"\"\"\n",
    "  # assumes detections are in detpath.format(classname)\n",
    "  # assumes annotations are in annopath.format(imagename)\n",
    "  # assumes imagesetfile is a text file with each line an image name\n",
    "  # cachedir caches the annotations in a pickle file\n",
    "\n",
    "  # first load gt\n",
    "  if not os.path.isdir(cachedir):\n",
    "    os.mkdir(cachedir)\n",
    "  cachefile = os.path.join(cachedir, '%s_annots.pkl' % imagesetfile)\n",
    "  # read list of images\n",
    "  with open(imagesetfile, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "  imagenames = [x.strip() for x in lines]\n",
    "\n",
    "  if not os.path.isfile(cachefile):\n",
    "    # load annotations\n",
    "    recs = {}\n",
    "    for i, imagename in enumerate(imagenames):\n",
    "      recs[imagename] = parse_rec(annopath.format(imagename))\n",
    "      if i % 100 == 0:\n",
    "        print('Reading annotation for {:d}/{:d}'.format(\n",
    "          i + 1, len(imagenames)))\n",
    "    # save\n",
    "    #print('Saving cached annotations to {:s}'.format(cachefile))\n",
    "    #with open(cachefile, 'wb') as f:\n",
    "    #  pickle.dump(recs, f)\n",
    "  else:\n",
    "    # load\n",
    "    with open(cachefile, 'rb') as f:\n",
    "      try:\n",
    "        recs = pickle.load(f)\n",
    "      except:\n",
    "        recs = pickle.load(f, encoding='bytes')\n",
    "\n",
    "  # extract gt objects for this class\n",
    "  class_recs = {}\n",
    "  npos = 0\n",
    "  for imagename in imagenames:\n",
    "    R = [obj for obj in recs[imagename] if obj['name'] == classname]\n",
    "    bbox = np.array([x['bbox'] for x in R])\n",
    "    difficult = np.array([x['difficult'] for x in R]).astype(np.bool)\n",
    "    det = [False] * len(R)\n",
    "    npos = npos + sum(~difficult)\n",
    "    class_recs[imagename] = {'bbox': bbox,\n",
    "                             'difficult': difficult,\n",
    "                             'det': det}\n",
    "\n",
    "  # read dets\n",
    "  detfile = detpath.format(classname)\n",
    "  with open(detfile, 'r') as f:\n",
    "    lines = f.readlines()\n",
    "\n",
    "  splitlines = [x.strip().split(' ') for x in lines]\n",
    "  image_ids = [x[0] for x in splitlines]\n",
    "  confidence = np.array([float(x[1]) for x in splitlines])\n",
    "  BB = np.array([[float(z) for z in x[2:]] for x in splitlines])\n",
    "\n",
    "  nd = len(image_ids)\n",
    "  tp = np.zeros(nd)\n",
    "  fp = np.zeros(nd)\n",
    "\n",
    "  if BB.shape[0] > 0:\n",
    "    # sort by confidence\n",
    "    sorted_ind = np.argsort(-confidence)\n",
    "#    sorted_scores = np.sort(-confidence)\n",
    "    BB = BB[sorted_ind, :]\n",
    "    image_ids = [image_ids[x] for x in sorted_ind]\n",
    "\n",
    "    # go down dets and mark TPs and FPs\n",
    "    for d in range(nd):\n",
    "      id = image_ids[d][-10:-4]\n",
    "      #print(id)\n",
    "      # catch bad detections\n",
    "      try:\n",
    "          R = class_recs[id]\n",
    "      except:\n",
    "        #print(\"det not found\")\n",
    "        continue\n",
    "        \n",
    "      bb = BB[d, :].astype(float)\n",
    "      ovmax = -np.inf\n",
    "      BBGT = R['bbox'].astype(float)\n",
    "\n",
    "      if BBGT.size > 0:\n",
    "        # compute overlaps\n",
    "        # intersection\n",
    "        ixmin = np.maximum(BBGT[:, 0], bb[0])\n",
    "        iymin = np.maximum(BBGT[:, 1], bb[1])\n",
    "        ixmax = np.minimum(BBGT[:, 2], bb[2])\n",
    "        iymax = np.minimum(BBGT[:, 3], bb[3])\n",
    "        iw = np.maximum(ixmax - ixmin + 1., 0.)\n",
    "        ih = np.maximum(iymax - iymin + 1., 0.)\n",
    "        inters = iw * ih\n",
    "\n",
    "        # union\n",
    "        uni = ((bb[2] - bb[0] + 1.) * (bb[3] - bb[1] + 1.) +\n",
    "               (BBGT[:, 2] - BBGT[:, 0] + 1.) *\n",
    "               (BBGT[:, 3] - BBGT[:, 1] + 1.) - inters)\n",
    "\n",
    "        overlaps = inters / uni\n",
    "        ovmax = np.max(overlaps)\n",
    "        jmax = np.argmax(overlaps)\n",
    "\n",
    "      if ovmax > ovthresh:\n",
    "        if not R['difficult'][jmax]:\n",
    "          if not R['det'][jmax]:\n",
    "            tp[d] = 1.\n",
    "            R['det'][jmax] = 1\n",
    "          else:\n",
    "            fp[d] = 1.\n",
    "      else:\n",
    "        fp[d] = 1.\n",
    "\n",
    "  # compute precision recall\n",
    "  fp = np.cumsum(fp)\n",
    "  tp = np.cumsum(tp)\n",
    "  rec = tp / float(npos)\n",
    "  # avoid divide by zero in case the first detection matches a difficult\n",
    "  # ground truth\n",
    "  prec = tp / np.maximum(tp + fp, np.finfo(np.float64).eps)\n",
    "  ap = voc_ap(rec, prec, use_07_metric)\n",
    "\n",
    "  return rec, prec, ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pascal_classes = np.asarray(['aeroplane', 'bicycle', 'bird', 'boat',\n",
    "               'bottle', 'bus', 'car', 'cat', 'chair',\n",
    "               'cow', 'diningtable', 'dog', 'horse',\n",
    "               'motorbike', 'person', 'pottedplant',\n",
    "               'sheep', 'sofa', 'train', 'tvmonitor'])\n",
    "PASCAL_CLASSES = pascal_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# write out detections for evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "def get_voc_results_file_template(cls):\n",
    "        # VOCdevkit/results/VOC2007/Main/<comp_id>_det_test_aeroplane.txt\n",
    "        filename = 'det_' + \"val\" + '_'+cls+'.txt'\n",
    "        filedir = os.path.join(DEVKIT_PATH, 'results', 'VOC2007', 'Main')\n",
    "        if not os.path.exists(filedir):\n",
    "            os.makedirs(filedir)\n",
    "        path = os.path.join(filedir, filename)\n",
    "        return path\n",
    "\n",
    "\n",
    "def write_voc_results_file(pascal_classes, all_boxes, image_index):\n",
    "        for cls_ind, cls in enumerate(pascal_classes):\n",
    "            if cls == '__background__':\n",
    "                continue\n",
    "            print('Writing {} VOC results file'.format(cls))\n",
    "            filename = get_voc_results_file_template(cls)\n",
    "            with open(filename, 'wt') as f:\n",
    "                for im_ind, index in enumerate(image_index):\n",
    "                    dets = np.asarray(all_boxes[cls_ind][im_ind])\n",
    "                    if dets == []:\n",
    "                        continue\n",
    "                    # the VOCdevkit expects 1-based indices\n",
    "                    for k in range(dets.shape[0]):\n",
    "                        #print(dets[k, 0])\n",
    "                        f.write('{:s} {:.3f} {:.1f} {:.1f} {:.1f} {:.1f}\\n'.\n",
    "                                format(index, dets[k, 0],\n",
    "                                       dets[k, 1] + 1, dets[k, 2] + 1,\n",
    "                                       dets[k, 3] + 1, dets[k, 4] + 1))\n",
    "import xml.etree.ElementTree as ET\n",
    "def parse_rec(filename):\n",
    "  \"\"\" Parse a PASCAL VOC xml file \"\"\"\n",
    "  tree = ET.parse(filename)\n",
    "  objects = []\n",
    "  for obj in tree.findall('object'):\n",
    "    obj_struct = {}\n",
    "    obj_struct['name'] = obj.find('name').text\n",
    "    obj_struct['pose'] = obj.find('pose').text\n",
    "    obj_struct['truncated'] = int(obj.find('truncated').text)\n",
    "    obj_struct['difficult'] = int(obj.find('difficult').text)\n",
    "    bbox = obj.find('bndbox')\n",
    "    obj_struct['bbox'] = [int(bbox.find('xmin').text),\n",
    "                          int(bbox.find('ymin').text),\n",
    "                          int(bbox.find('xmax').text),\n",
    "                          int(bbox.find('ymax').text)]\n",
    "    objects.append(obj_struct)\n",
    "\n",
    "  return objects\n",
    "def voc_ap(rec, prec, use_07_metric=False):\n",
    "  \"\"\" ap = voc_ap(rec, prec, [use_07_metric])\n",
    "  Compute VOC AP given precision and recall.\n",
    "  If use_07_metric is true, uses the\n",
    "  VOC 07 11 point method (default:False).\n",
    "  \"\"\"\n",
    "  if use_07_metric:\n",
    "    # 11 point metric\n",
    "    ap = 0.\n",
    "    for t in np.arange(0., 1.1, 0.1):\n",
    "      if np.sum(rec >= t) == 0:\n",
    "        p = 0\n",
    "      else:\n",
    "        p = np.max(prec[rec >= t])\n",
    "      ap = ap + p / 11.\n",
    "  else:\n",
    "    # correct AP calculation\n",
    "    # first append sentinel values at the end\n",
    "    mrec = np.concatenate(([0.], rec, [1.]))\n",
    "    mpre = np.concatenate(([0.], prec, [0.]))\n",
    "\n",
    "    # compute the precision envelope\n",
    "    for i in range(mpre.size - 1, 0, -1):\n",
    "      mpre[i - 1] = np.maximum(mpre[i - 1], mpre[i])\n",
    "\n",
    "    # to calculate area under PR curve, look for points\n",
    "    # where X axis (recall) changes value\n",
    "    i = np.where(mrec[1:] != mrec[:-1])[0]\n",
    "\n",
    "    # and sum (\\Delta recall) * prec\n",
    "    ap = np.sum((mrec[i + 1] - mrec[i]) * mpre[i + 1])\n",
    "  return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm ../VOCdevkit/results/VOC2007/Main/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "write_voc_results_file(pascal_classes, all_boxes, val_img_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def python_eval(output_dir='output'):\n",
    "        annopath = os.path.join(\n",
    "            DEVKIT_PATH,\n",
    "            'VOC2007',\n",
    "            'Annotations',\n",
    "            '{:s}.xml')\n",
    "        imagesetfile = os.path.join(\n",
    "            DEVKIT_PATH,\n",
    "            'VOC2007',\n",
    "            'ImageSets',\n",
    "            'Main',\n",
    "            SET + '.txt')\n",
    "        cachedir = os.path.join(DEVKIT_PATH, 'annotations_cache')\n",
    "        aps = []\n",
    "        # The PASCAL VOC metric changed in 2010.\n",
    "        # VOC07 metric is quite old so don't use.\n",
    "        use_07_metric = False\n",
    "        print('VOC07 metric? ' + ('Yes' if use_07_metric else 'No'))\n",
    "        if not os.path.isdir(output_dir):\n",
    "            os.mkdir(output_dir)\n",
    "        for i, cls in enumerate(PASCAL_CLASSES):\n",
    "            if cls == '__background__':\n",
    "                continue\n",
    "            filename = get_voc_results_file_template(cls)\n",
    "            rec, prec, ap = voc_eval(\n",
    "                filename, annopath, imagesetfile, cls, cachedir, ovthresh=0.5,\n",
    "                use_07_metric=use_07_metric)\n",
    "            aps += [ap]\n",
    "            print('AP for {} = {:.4f}'.format(cls, ap))\n",
    "            with open(os.path.join(output_dir, cls + '_pr.pkl'), 'wb') as f:\n",
    "                pickle.dump({'rec': rec, 'prec': prec, 'ap': ap}, f)\n",
    "        print('Mean AP = {:.4f}'.format(np.mean(aps)))\n",
    "        print('~~~~~~~~')\n",
    "        print('Results:')\n",
    "        for ap in aps:\n",
    "            print('{:.3f}'.format(ap))\n",
    "        print('{:.3f}'.format(np.mean(aps)))\n",
    "        print('~~~~~~~~')\n",
    "        print('')\n",
    "        print('--------------------------------------------------------------')\n",
    "        print('Results computed with the **unofficial** Python eval code.')\n",
    "        print('Results should be very close to the official MATLAB eval code.')\n",
    "        print('Recompute with `./tools/reval.py --matlab ...` for your paper.')\n",
    "        print('-- Thanks, The Management')\n",
    "        print('--------------------------------------------------------------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate detections\n",
    "python_eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
